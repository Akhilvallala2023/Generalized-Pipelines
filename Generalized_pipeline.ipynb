{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQeBYVSk1/OuXoJ1+6xjd9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akhilvallala2023/Generalized-Pipelines/blob/main/Generalized_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt the user to enter their GitHub token securely\n",
        "import getpass\n",
        "\n",
        "# Prompt for the token\n",
        "token = getpass.getpass(\"Enter your GitHub token: \")\n",
        "\n",
        "# Clone the repository using the token\n",
        "!git clone https://{token}@github.com/Akhilvallala2023/Generalized-Pipelines.git\n"
      ],
      "metadata": {
        "id": "z21iKXU84_bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWgfL4Q8g_lT"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade langchain_openai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Resume"
      ],
      "metadata": {
        "id": "7hSnTTbEl8Yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Prompt the user to enter the OpenAI API key\n",
        "api_key = input(\"Please enter your OpenAI API key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# Confirm that the key has been set\n",
        "print(\"OpenAI API key has been set.\")\n",
        "\n",
        "# Initialize the model\n",
        "model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "# Step 1: Define General Agent Templates\n",
        "\n",
        "# Task Analysis Agent - identifies task-specific requirements\n",
        "task_prompt = ChatPromptTemplate.from_template(\"Analyze the task: {task}\")\n",
        "task_analysis_chain = LLMChain(prompt=task_prompt, llm=model)\n",
        "\n",
        "# Configuration Agent - configures agents based on task analysis\n",
        "config_prompt = ChatPromptTemplate.from_template(\"Based on analysis {analysis}, configure agents\")\n",
        "config_chain = LLMChain(prompt=config_prompt, llm=model)\n",
        "\n",
        "# Execution Agent for generating resume content\n",
        "resume_generation_prompt = ChatPromptTemplate.from_template(\"Create a professional resume for the user with the following details: {user_input}\")\n",
        "resume_generation_chain = LLMChain(prompt=resume_generation_prompt, llm=model)\n",
        "\n",
        "# ATS Scoring Agent for evaluating resume against job description\n",
        "ats_score_prompt = ChatPromptTemplate.from_template(\"Evaluate the resume against the job description and provide an ATS score.\")\n",
        "ats_score_chain = LLMChain(prompt=ats_score_prompt, llm=model)\n",
        "\n",
        "# Step 2: Agent Orchestrator for Dynamic Task Assembly\n",
        "class DynamicAgentOrchestrator:\n",
        "    def __init__(self, task_description: str, user_input: str = None, job_description: str = None):\n",
        "        self.task_description = task_description\n",
        "        self.user_input = user_input\n",
        "        self.job_description = job_description\n",
        "        self.agents = {}\n",
        "\n",
        "    def analyze_task(self):\n",
        "        # Analyze the task and determine required agents\n",
        "        analysis_result = task_analysis_chain.invoke({\"task\": self.task_description})\n",
        "        return analysis_result\n",
        "\n",
        "    def configure_agents(self, analysis_result):\n",
        "        # Configure agents dynamically based on task analysis\n",
        "        config_result = config_chain.invoke({\"analysis\": analysis_result})\n",
        "\n",
        "        # Dynamically define agents based on task requirements\n",
        "        if \"resume\" in self.task_description.lower():\n",
        "            self.agents[\"resume_generator\"] = resume_generation_chain\n",
        "            self.agents[\"ats_scorer\"] = ats_score_chain\n",
        "\n",
        "    def run_agents(self):\n",
        "        # Run the dynamically configured agents\n",
        "\n",
        "        # Execute resume generation if configured\n",
        "        resume_output = None\n",
        "        if \"resume_generator\" in self.agents:\n",
        "            resume_output = self.agents[\"resume_generator\"].invoke({\"user_input\": self.user_input})\n",
        "\n",
        "        # Execute ATS scoring if configured\n",
        "        ats_score_output = None\n",
        "        if \"ats_scorer\" in self.agents and resume_output:\n",
        "            ats_score_output = self.agents[\"ats_scorer\"].invoke({\"output\": resume_output, \"job_description\": self.job_description})\n",
        "\n",
        "        # Collect results\n",
        "        return {\n",
        "            \"generated_resume\": resume_output,\n",
        "            \"ats_score\": ats_score_output\n",
        "        }\n",
        "\n",
        "# Step 3: Run Dynamic Task Workflow\n",
        "def execute_dynamic_task(task_description: str, user_input: str, job_description: str):\n",
        "    # Initialize the orchestrator with the task, user input, and job description\n",
        "    orchestrator = DynamicAgentOrchestrator(task_description, user_input, job_description)\n",
        "\n",
        "    # Step 1: Task Analysis\n",
        "    analysis_result = orchestrator.analyze_task()\n",
        "\n",
        "    # Step 2: Agent Configuration\n",
        "    orchestrator.configure_agents(analysis_result)\n",
        "\n",
        "    # Step 3: Execute Agents and Collect Results\n",
        "    results = orchestrator.run_agents()\n",
        "\n",
        "    # Print only the resume and ATS score\n",
        "    print(json.dumps({\n",
        "        \"generated_resume\": results[\"generated_resume\"],\n",
        "        \"ats_score\": results[\"ats_score\"]\n",
        "    }, indent=2))\n",
        "\n",
        "# Example usage\n",
        "task_description = \"Create a resume for the user.\"\n",
        "user_input = \"User has 5 years experience in software development, skilled in Python, AI, and data science.\"\n",
        "job_description = \"Looking for a software developer with experience in Python and machine learning.\"\n",
        "\n",
        "# Run the task\n",
        "execute_dynamic_task(task_description, user_input, job_description)\n"
      ],
      "metadata": {
        "id": "57iOdYNThYzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Working Pipeline (done)"
      ],
      "metadata": {
        "id": "_DcoEOqy6JYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Prompt the user to enter the OpenAI API key\n",
        "api_key = input(\"Please enter your OpenAI API key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# Confirm that the key has been set\n",
        "print(\"OpenAI API key has been set.\")\n",
        "\n",
        "# Initialize the model (use GPT-3.5 or GPT-4)\n",
        "model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "# Define an Agent class where each agent's behavior is defined by a prompt\n",
        "class Agent:\n",
        "    def __init__(self, name: str, prompt: str):\n",
        "        self.name = name\n",
        "        # Use '{input}' as a placeholder in the prompt to ensure the data is passed correctly\n",
        "        formatted_prompt = f\"{prompt} Input: {{input}}\"\n",
        "        self.prompt_template = ChatPromptTemplate.from_template(formatted_prompt)\n",
        "        self.chain = LLMChain(prompt=self.prompt_template, llm=model)\n",
        "\n",
        "    def run(self, data):\n",
        "        print(f\"\\nRunning agent '{self.name}'...\")\n",
        "        # Pass 'data' as 'input' in the invocation\n",
        "        response = self.chain.invoke({\"input\": data})\n",
        "        # Ensure output key is 'output' for clarity and only the essential data is passed\n",
        "        return {\"input\": data, \"output\": response[\"text\"]}\n",
        "\n",
        "# Define a Pipeline to manage multiple agents\n",
        "class Pipeline:\n",
        "    def __init__(self):\n",
        "        self.agents = []\n",
        "\n",
        "    def add_agent(self, agent: Agent):\n",
        "        self.agents.append(agent)\n",
        "        print(f\"Agent '{agent.name}' added to the pipeline.\")\n",
        "\n",
        "    def run_pipeline(self, data):\n",
        "        print(\"\\nStarting pipeline execution...\\n\")\n",
        "        # Pass 'data' through each agent in the pipeline sequentially\n",
        "        for i, agent in enumerate(self.agents, start=1):\n",
        "            # The output of each agent becomes the input for the next agent\n",
        "            result = agent.run(data)\n",
        "            data = result[\"output\"]  # Only pass the 'output' to the next agent\n",
        "            # Display the output of each agent with clear formatting\n",
        "            print(f\"Output after Agent {i} ('{agent.name}'): {result}\\n\")\n",
        "            time.sleep(1)  # Adding a slight delay for readability\n",
        "        print(\"Pipeline execution completed.\")\n",
        "        # Final output after passing through all agents\n",
        "        return data\n",
        "\n",
        "# Helper function to create a new agent\n",
        "def create_agent():\n",
        "    name = input(\"Enter the name of the new agent: \")\n",
        "    prompt = input(\"Describe the agent's task in natural language (e.g., 'Translate the English text to Telugu'): \")\n",
        "    return Agent(name, prompt)\n",
        "\n",
        "# Main loop to manage pipeline creation\n",
        "def main():\n",
        "    pipeline = Pipeline()\n",
        "\n",
        "    while True:\n",
        "        if len(pipeline.agents) >= 1:\n",
        "            choice = input(\"\\nWould you like to add a new agent or run the pipeline? (add/run/exit): \").strip().lower()\n",
        "        else:\n",
        "            choice = 'add'  # Force adding the first agent\n",
        "\n",
        "        if choice == 'add':\n",
        "            agent = create_agent()\n",
        "            pipeline.add_agent(agent)\n",
        "        elif choice == 'run' and len(pipeline.agents) >= 1:\n",
        "            # Allow the pipeline to run even if there's only one agent\n",
        "            initial_data = input(\"Enter initial data for the pipeline: \")\n",
        "            pipeline.run_pipeline(initial_data)\n",
        "        elif choice == 'exit':\n",
        "            print(\"Exiting...\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid option. Please enter 'add', 'run', or 'exit'.\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "_D8lPzhW1uRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Input file to the pipeline"
      ],
      "metadata": {
        "id": "nTM6RMhz6lTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-pptx PyMuPDF python-docx\n"
      ],
      "metadata": {
        "id": "aaaIQlu6HG_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import os\n",
        "import time\n",
        "from pptx import Presentation  # For reading .pptx files\n",
        "import fitz  # For reading .pdf files (PyMuPDF)\n",
        "from docx import Document  # For reading .docx files\n",
        "\n",
        "# Prompt the user to enter the OpenAI API key\n",
        "api_key = input(\"Please enter your OpenAI API key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# Confirm that the key has been set\n",
        "print(\"OpenAI API key has been set.\")\n",
        "\n",
        "# Initialize the model (use GPT-3.5 or GPT-4)\n",
        "model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "# Define an Agent class where each agent's behavior is defined by a prompt\n",
        "class Agent:\n",
        "    def __init__(self, name: str, prompt: str):\n",
        "        self.name = name\n",
        "        # Use '{input}' as a placeholder in the prompt to ensure the data is passed correctly\n",
        "        formatted_prompt = f\"{prompt} Input: {{input}}\"\n",
        "        self.prompt_template = ChatPromptTemplate.from_template(formatted_prompt)\n",
        "        self.chain = LLMChain(prompt=self.prompt_template, llm=model)\n",
        "\n",
        "    def run(self, data):\n",
        "        print(f\"\\nRunning agent '{self.name}'...\")\n",
        "        # Pass 'data' as 'input' in the invocation\n",
        "        response = self.chain.invoke({\"input\": data})\n",
        "        # Ensure output key is 'output' for clarity and only the essential data is passed\n",
        "        return {\"input\": data, \"output\": response[\"text\"]}\n",
        "\n",
        "# Define a Pipeline to manage multiple agents\n",
        "class Pipeline:\n",
        "    def __init__(self):\n",
        "        self.agents = []\n",
        "\n",
        "    def add_agent(self, agent: Agent):\n",
        "        self.agents.append(agent)\n",
        "        print(f\"Agent '{agent.name}' added to the pipeline.\")\n",
        "\n",
        "    def run_pipeline(self, data):\n",
        "        print(\"\\nStarting pipeline execution...\\n\")\n",
        "        # Pass 'data' through each agent in the pipeline sequentially\n",
        "        for i, agent in enumerate(self.agents, start=1):\n",
        "            # The output of each agent becomes the input for the next agent\n",
        "            result = agent.run(data)\n",
        "            data = result[\"output\"]  # Only pass the 'output' to the next agent\n",
        "            # Display the output of each agent with clear formatting\n",
        "            print(f\"Output after Agent {i} ('{agent.name}'): {result}\\n\")\n",
        "            time.sleep(1)  # Adding a slight delay for readability\n",
        "        print(\"Pipeline execution completed.\")\n",
        "        # Final output after passing through all agents\n",
        "        return data\n",
        "\n",
        "# Helper function to create a new agent\n",
        "def create_agent():\n",
        "    name = input(\"Enter the name of the new agent: \")\n",
        "    prompt = input(\"Describe the agent's task in natural language (e.g., 'Translate the English text to Telugu'): \")\n",
        "    return Agent(name, prompt)\n",
        "\n",
        "# Function to read different types of files and return their content\n",
        "def read_file(file_path):\n",
        "    print(\"file_path =\",file_path)\n",
        "    extension = os.path.splitext(file_path)[-1].lower()\n",
        "\n",
        "    try:\n",
        "        if extension == \".txt\":\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                return file.read()\n",
        "\n",
        "        elif extension == \".pptx\":\n",
        "            presentation = Presentation(file_path)\n",
        "            text = \"\"\n",
        "            for slide in presentation.slides:\n",
        "                for shape in slide.shapes:\n",
        "                    if hasattr(shape, \"text\"):\n",
        "                        text += shape.text + \"\\n\"\n",
        "            return text\n",
        "\n",
        "        elif extension == \".pdf\":\n",
        "            text = \"\"\n",
        "            pdf_document = fitz.open(file_path)\n",
        "            for page_num in range(pdf_document.page_count):\n",
        "                page = pdf_document[page_num]\n",
        "                text += page.get_text(\"text\") + \"\\n\"\n",
        "            pdf_document.close()\n",
        "            return text\n",
        "\n",
        "        elif extension == \".docx\":\n",
        "            doc = Document(file_path)\n",
        "            text = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
        "            return text\n",
        "\n",
        "        else:\n",
        "            print(f\"Unsupported file type: {extension}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_pipeline_input():\n",
        "    input_type = input(\"Enter input type (text/file): \").strip().lower()\n",
        "\n",
        "    if input_type == \"text\":\n",
        "        # Text input\n",
        "        return input(\"Enter text input: \")\n",
        "    elif input_type == \"file\":\n",
        "        # File input\n",
        "        file_path = input(\"Enter the file path: \").strip()\n",
        "        return read_file(file_path)\n",
        "    else:\n",
        "        print(\"Invalid input type. Please enter 'text' or 'file'.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Main loop to manage pipeline creation\n",
        "def main():\n",
        "    pipeline = Pipeline()\n",
        "\n",
        "    while True:\n",
        "        if len(pipeline.agents) >= 1:\n",
        "            choice = input(\"\\nWould you like to add a new agent or run the pipeline? (add/run/exit): \").strip().lower()\n",
        "        else:\n",
        "            choice = 'add'  # Force adding the first agent\n",
        "\n",
        "        if choice == 'add':\n",
        "            agent = create_agent()\n",
        "            pipeline.add_agent(agent)\n",
        "        elif choice == 'run' and len(pipeline.agents) >= 1:\n",
        "            # Allow the pipeline to run even if there's only one agent\n",
        "            initial_data = get_pipeline_input()\n",
        "            if initial_data is not None:\n",
        "                pipeline.run_pipeline(initial_data)\n",
        "            else:\n",
        "                print(\"No valid input provided. Pipeline execution aborted.\")\n",
        "        elif choice == 'exit':\n",
        "            print(\"Exiting...\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid option. Please enter 'add', 'run', or 'exit'.\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "tHW94BhvFDIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Input file at the agent level"
      ],
      "metadata": {
        "id": "QrLftNDn6xmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Making a proper prompt"
      ],
      "metadata": {
        "id": "F26FmZ6E6ek5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#History of pipelines"
      ],
      "metadata": {
        "id": "0tIdsjEl6bX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Making pipeline public or Private"
      ],
      "metadata": {
        "id": "v5ek3S8669BO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rating to the pipeline"
      ],
      "metadata": {
        "id": "9wXkI-PN7GtP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-dtQDBVd3arw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}